{"meta":{"title":"Suave's Blog","subtitle":"","description":"","author":"Suave","url":"https://suavess.github.io","root":"/"},"pages":[{"title":"","date":"2022-06-16T02:01:19.362Z","updated":"2022-06-16T02:01:19.362Z","comments":true,"path":"404.html","permalink":"https://suavess.github.io/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"所有分类","date":"2022-06-16T02:01:19.366Z","updated":"2022-06-16T02:01:19.366Z","comments":true,"path":"categories/index.html","permalink":"https://suavess.github.io/categories/index.html","excerpt":"","text":""},{"title":"我的朋友们","date":"2022-06-16T02:01:19.366Z","updated":"2022-06-16T02:01:19.366Z","comments":true,"path":"friends/index.html","permalink":"https://suavess.github.io/friends/index.html","excerpt":"这里写友链上方的内容。","text":"这里写友链上方的内容。 这里可以写友链页面下方的文字备注，例如自己的友链规范、示例等。"},{"title":"","date":"2022-06-16T02:01:19.366Z","updated":"2022-06-16T02:01:19.366Z","comments":true,"path":"about/index.html","permalink":"https://suavess.github.io/about/index.html","excerpt":"","text":""},{"title":"","date":"2022-06-16T02:01:19.366Z","updated":"2022-06-16T02:01:19.366Z","comments":true,"path":"mylist/index.html","permalink":"https://suavess.github.io/mylist/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2022-06-16T02:01:19.367Z","updated":"2022-06-16T02:01:19.367Z","comments":true,"path":"tags/index.html","permalink":"https://suavess.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"分布式事务基础","slug":"分布式事务基础","date":"2022-10-05T05:44:19.000Z","updated":"2022-10-05T07:18:06.329Z","comments":true,"path":"2022/10/05/分布式事务基础/","link":"","permalink":"https://suavess.github.io/2022/10/05/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E5%9F%BA%E7%A1%80/","excerpt":"","text":"事务概念指的就是一个操作单元，在这个操作单元中的所有操作最终要保持一致的行为，要么所有操作都成功，要么所有的操作都被撤销。 通俗一点？举个生活中的例子：你去小卖铺买东西，“一手交钱，一手交货”就是一个事务的例子，交钱和交货必 须全部成功，事务才算成功，任一个活动失败，事务将撤销所有已成功的活动。 事务可以看做是一次大的操作，它由不同的小操作组成，这些操作要么全部成功，要么全部失败。 事务的四个特性（ACID） 原子性（Atomicity）：操作这些指令时，要么全部执行成功，要么全部不执行。只要其中一个指令执行失败，所有的指令都执行失败，数据进行回滚，回到执行指令前的数据状态。要么执行，要么不执行 一致性（Consistency）：事务的执行使数据从一个状态转换为另一个状态，数据库的完整性约束没有被破坏。能量守恒，总量不变 拿转账来说，假设用户A和用户B两者的钱加起来一共是2000，那么不管A和B之间如何转账，转几次账，事务结束后两个用户的钱相加起来应该还得是2000，这就是事务的一致性。 隔离性（Isolation）：隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。信息彼此独立，互不干扰 即要达到这么一种效果：对于任意两个并发的事务T1和T2，在事务T1看来，T2要么在T1开始之前就已经结束，要么在T1结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。 持久性（Durability）：当事务正确完成后，它对于数据的改变是永久性的。不会轻易丢失 例如我们在使用JDBC操作数据库时，在提交事务方法后，提示用户事务操作完成，当我们程序执行完成直到看到提示后，就可以认定事务以及正确提交，即使这时候数据库出现了问题，也必须要将我们的事务完全执行完成，否则就会造成我们看到提示事务处理完毕，但是数据库因为故障而没有执行事务的重大错误。 分布式事务随着互联网的快速发展，软件系统由原来的单体应用转变为分布式应用，下图描述了单体应用向微服务的演变：分布式系统会把一个应用系统拆分为可独立部署的多个服务，因此需要服务与服务之间远程协作才能完成事务操 作，这种分布式系统环境下由不同的服务之间通过网络远程协作完成事务称之为分布式事务，例如用户注册送积分 事务、创建订单减库存事务，银行转账事务等都是分布式事务。 典型的场景就是微服务架构 微服务之间通过远程调用完成事务操作。 比如：订单微服务和库存微服务，下单的 同时订单微服务请求库存微服务减库存。 简言之：跨JVM进程产生分布式事务。 分布式事务产生场景 跨JVM进程产生分布式事务 典型的场景就是微服务架构 微服务之间通过远程调用完成事务操作。 比如：订单微服务和库存微服务，下单的 同时订单微服务请求库存微服务减库存。 跨数据库实例 典型的场景就是微服务架构 微服务之间通过远程调用完成事务操作。 比如：订单微服务和库存微服务，下单的 同时订单微服务请求库存微服务减库存。 我们了解到了分布式事务的基础概念。与本地事务不同的是，分布式系统之所以叫分布式，是因 为提供服务的各个节点分布在不同机器上，相互之间通过网络交互。不能因为有一点网络问题就导致整个系统无法 提供服务，网络因素成为了分布式事务的考量标准之一。因此，分布式事务需要更进一步的理论支持，接下来，我们先来学习一下分布式事务的CAP理论。 CAP原则CAP原则又叫CAP定理，同时又被称作布鲁尔定理（Brewer’s theorem），指的是在一个分布式系统中，不可能同时满足以下三点。 一致性（Consistency）副本最新：指强一致性，在写操作完成后开始的任何读操作都必须返回该值，或者后续写操作的结果。 在一致性系统中，一旦客户端将值写入任何一台服务器并获得响应，那么之后client从其他任何服务器读取的都是刚写入的数据 一致性保证了不管向哪台服务器写入数据，其他的服务器能实时同步数据 可用性（Availability）高可用：可用性是指，每次向未崩溃的节点发送请求，总能保证收到响应数据（允许不是最新数据） 分区容忍性（Partition tolerance）能容忍网络分区：分布式系统在遇到任何网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务 服务器A和B发送给对方的任何消息都是可以放弃的，也就是说A和B可能因为各种意外情况，导致无法成功进行同步，分布式系统要能容忍这种情况。除非整个网络环境都发生了故障。 分布式事务协议 背景 在分布式系统里，每个节点都可以知晓自己操作的成功或者失败，却无法知道其他节点操作的成功或失败。当一个事务跨多个节点时，为了保持事务的原子性与一致性，而引入一个协调者来统一掌控所有参与者的操作结果，并指示它们是否要把操作结果进行真正的提交或者回滚（rollback）。 二阶段提交2PC二阶段提交协议（Two-phase Commit，即 2PC）是常用的分布式事务解决方案，即将事务的提交过程分为两个阶段来进行处理。 阶段 准备阶段 提交阶段 参与角色 协调者：事务的发起者 参与者：事务的执行者 第一阶段 协调者向所有参与者发送事务内容，询问是否可以提交事务，并等待答复 各参与者执行事务操作，将 undo 和 redo 信息记入事务日志中（但不提交事务） 如参与者执行成功，给协调者反馈同意，否则反馈中止 第二阶段当协调者节点从所有参与者节点获得的相应消息都为同意时： 协调者节点向所有参与者节点发出正式提交(commit)的请求。 参与者节点正式完成操作，并释放在整个事务期间内占用的资源。 参与者节点向协调者节点发送ack完成消息。 协调者节点收到所有参与者节点反馈的ack完成消息后，完成事务。 如果任一参与者节点在第一阶段返回的响应消息为中止，或者 协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时： 协调者节点向所有参与者节点发出回滚操作(rollback)的请求。 参与者节点利用阶段1写入的undo信息执行回滚，并释放在整个事务期间内占用的资源。 参与者节点向协调者节点发送ack回滚完成消息。 协调者节点受到所有参与者节点反馈的ack回滚完成消息后，取消事务。 不管最后结果如何，第二阶段都会结束当前事务。 二阶段提交看起来确实能够提供原子性的操作，但是不幸的是，二阶段提交还是有几个缺点的： 1. **性能问题**：执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。 1. **可靠性问题**：参与者发生故障。协调者需要给每个参与者额外指定超时机制，超时后整个事务失败。协调者发生故障。参与者会一直阻塞下去。需要额外的备机进行容错。 1. **可靠性问题**：参与者发生故障。协调者需要给每个参与者额外指定超时机制，超时后整个事务失败。协调者发生故障。参与者会一直阻塞下去。需要额外的备机进行容错。 优点 尽量保证了数据的强一致，适合对数据强一致要求很高的关键领域。（其实也不能100%保证强一致） 缺点 实现复杂，牺牲了可用性，对性能影响较大，不适合高并发高性能场景。 分布式事务解决方案 TCC 全局消息 基于可靠消息服务的分布式事务 最大努力通知 事务补偿（TCC）TCC方案是一种应用层面侵入业务的两阶段提交。是目前最火的一种柔性事务方案，其核心思想是：针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作 第一阶段 ​ Try（尝试）：主要是对业务系统做检测及资源预留 (加锁，锁住资源) 第二阶段 ​ 本阶段根据第一阶段的结果，决定是执行confirm还是cancel ​ Confirm（确认）：执行真正的业务执行业务，释放锁 ​ Cancle（取消）：是预留资源的取消出问题，释放锁 案例为了方便理解，下面以电商下单为例进行方案解析，这里把整个过程简单分为扣减库存，订单创建 2 个步骤，库存服务和订单服务分别在不同的服务器节点上。 假设商品库存为 100，购买数量为 2，这里检查和更新库存的同时，冻结用户购买数量的库存，同时创建订单，订单状态为待确认。 Try阶段 ​ TCC 机制中的 Try 仅是一个初步操作，它和后续的确认一起才能真正构成一个完整的业务逻辑，这个阶段主要完成： 完成所有业务检查( 一致性 ) 。 预留必须业务资源( 准隔离性 ) 。 Try 尝试执行业务。 Confirm / Cancel 阶段 ​ 根据 Try 阶段服务是否全部正常执行，继续执行确认操作（Confirm）或取消操作（Cancel）。 ​ Confirm 和 Cancel 操作满足幂等性，如果 Confirm 或 Cancel 操作执行失败，将会不断重试直到执行完成。 ​ Confirm：当 Try 阶段服务全部正常执行， 执行确认业务逻辑操作 ​ 这里使用的资源一定是 Try 阶段预留的业务资源。在 TCC 事务机制中认为，如果在 Try 阶段能正常的预留资源，那 Confirm 一定能完整正确的提交。 ​ Confirm 阶段也可以看成是对 Try 阶段的一个补充，Try+Confirm 一起组成了一个完整的业务逻辑。 ​ Cancel：当 Try 阶段存在服务执行失败， 进入 Cancel 阶段 Cancel 取消执行，释放 Try 阶段预留的业务资源，上面的例子中，Cancel 操作会把冻结的库存释放，并更新订单状态为取消。 最终一致性保证 TCC 事务机制以初步操作（Try）为中心的，确认操作（Confirm）和取消操作（Cancel）都是围绕初步操作（Try）而展开。因此，Try 阶段中的操作，其保障性是最好的，即使失败，仍然有取消操作（Cancel）可以将其执行结果撤销。 Try阶段执行成功并开始执行 Confirm阶段时，默认 Confirm阶段是不会出错的。也就是说只要Try成功，Confirm一定成功TCC设计之初的定义 。 Confirm与Cancel如果失败，由TCC框架进行==重试==补偿 存在极低概率在CC环节彻底失败，则需要定时任务或人工介入 方案总结TCC 事务机制相对于传统事务机制（X/Open XA），TCC 事务机制相比于上面介绍的 XA 事务机制，有以下优点： 性能提升：具体业务来实现控制资源锁的粒度变小，不会锁定整个资源。 数据最终一致性：基于 Confirm 和 Cancel 的幂等性，保证事务最终完成确认或者取消，保证数据的一致性。 可靠性：解决了 XA 协议的协调者单点故障问题，由主业务方发起并控制整个业务活动，业务活动管理器也变成多点，引入集群。 缺点： TCC 的 Try、Confirm 和 Cancel 操作功能要按具体业务来实现，业务耦合度较高，提高了开发成本。","categories":[{"name":"分布式","slug":"分布式","permalink":"https://suavess.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://suavess.github.io/tags/Java/"},{"name":"分布式","slug":"分布式","permalink":"https://suavess.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"事务","slug":"事务","permalink":"https://suavess.github.io/tags/%E4%BA%8B%E5%8A%A1/"}]},{"title":"Java中transient关键字的使用","slug":"Java中transient关键字的使用","date":"2022-06-28T07:48:00.000Z","updated":"2022-06-28T09:27:48.740Z","comments":true,"path":"2022/06/28/Java中transient关键字的使用/","link":"","permalink":"https://suavess.github.io/2022/06/28/Java%E4%B8%ADtransient%E5%85%B3%E9%94%AE%E5%AD%97%E7%9A%84%E4%BD%BF%E7%94%A8/","excerpt":"","text":"transient的作用及使用方法​ 我们都知道一个对象只要实现了Serilizable接口，这个对象就可以被序列化，java的这种序列化模式为开发者提供了很多便利，我们可以不必关系具体序列化的过程，只要这个类实现了Serilizable接口，这个类的所有属性和方法都会自动序列化。 然而在实际开发过程中，常常会遇到这样的问题，这个类的有些属性需要序列化，而其他属性不需要被序列化。如果一个用户有一些敏感信息（如密码，银行卡号等），为了安全起见，不希望在网络操作（主要涉及到序列化操作，本地序列化缓存也适用）中被传输，这些信息对应的变量就可以加上transient关键字。换句话说，这个字段的生命周期仅存于调用者的内存中而不会写到磁盘里持久化。 总之，java 的transient关键字为我们提供了便利，你只需要实现Serilizable接口，将不需要序列化的属性前添加关键字transient，序列化对象的时候，这个属性就不会序列化到指定的目的地中。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package com.example.demo;import java.io.IOException;import java.io.ObjectInputStream;import java.io.ObjectOutputStream;import java.io.Serializable;import java.nio.file.Files;import java.nio.file.Paths;/** * @author Suave * @date 2022/6/16 10:18 */public class Test &#123; public static void main(String[] args) &#123; User user = new User(); user.setUsername(&quot;Suave&quot;); user.setPassword(&quot;123456&quot;); System.out.println(&quot;read before Serializable: &quot;); System.out.println(&quot;username: &quot; + user.getUsername()); System.out.println(&quot;password: &quot; + user.getPassword()); try &#123; ObjectOutputStream os = new ObjectOutputStream( Files.newOutputStream(Paths.get(&quot;/Users/suave/test.txt&quot;))); os.writeObject(user); os.flush(); os.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; ObjectInputStream is = new ObjectInputStream(Files.newInputStream(Paths.get(&quot;/Users/suave/test.txt&quot;))); user = (User) is.readObject(); is.close(); System.out.println(&quot;read after Serializable: &quot;); System.out.println(&quot;username: &quot; + user.getUsername()); System.out.println(&quot;password: &quot; + user.getPassword()); &#125; catch (IOException | ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125;&#125;class User implements Serializable &#123; private static final long serialVersionUID = 8294180014912103005L; private String username; private transient String password; public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125;&#125; 输出结果为 123456read before Serializable: username: Suavepassword: 123456read after Serializable: username: Suavepassword: null 密码字段为null，说明反序列化时根本没有从文件中获取到信息。 transient使用总结 一旦变量被transient修饰，变量将不再是对象持久化的一部分，该变量内容在序列化后无法获得访问。 transient关键字只能修饰变量，而不能修饰方法和类。注意，本地变量是不能被transient关键字修饰的。变量如果是用户自定义类变量，则该类需要实现Serializable接口。 被transient关键字修饰的变量不再能被序列化，一个静态变量不管是否被transient修饰，均不能被序列化。 第三点可能有些迷惑，因为发现在User类中的username字段前加上static关键字后，程序运行结果依然不变，即static类型的username也读出来为“Suave”了，这不与第三点说的矛盾吗？实际上是这样的：第三点确实没错（一个静态变量不管是否被transient修饰，均不能被序列化），反序列化后类中static型变量username的值为当前JVM中对应static变量的值，这个值是JVM中的不是反序列化得出的，可以来简单证明一下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class Test &#123; public static void main(String[] args) &#123; User user = new User(); user.setUsername(&quot;Suave&quot;); user.setPassword(&quot;123456&quot;); System.out.println(&quot;read before Serializable: &quot;); System.out.println(&quot;username: &quot; + user.getUsername()); System.out.println(&quot;password: &quot; + user.getPassword()); try &#123; ObjectOutputStream os = new ObjectOutputStream( Files.newOutputStream(Paths.get(&quot;/Users/suave/test.txt&quot;))); os.writeObject(user); os.flush(); os.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; User.username = &quot;abcde&quot;; ObjectInputStream is = new ObjectInputStream(Files.newInputStream(Paths.get(&quot;/Users/suave/test.txt&quot;))); user = (User) is.readObject(); is.close(); System.out.println(&quot;\\nread after Serializable: &quot;); System.out.println(&quot;username: &quot; + user.getUsername()); System.out.println(&quot;password: &quot; + user.getPassword()); &#125; catch (IOException | ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125;&#125;class User implements Serializable &#123; private static final long serialVersionUID = 8294180014912103005L; public static String username; private transient String password; public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125;&#125; 运行结果: 1234567read before Serializable: username: Suavepassword: 123456read after Serializable: username: abcdepassword: null 这说明反序列化后类中static型变量username的值为当前JVM中对应static变量的值，为修改后jmwang，而不是序列化时的值Alexia。 被transient关键字修饰的变量真的不能被序列化吗？举个🌰 1234567891011121314151617181920212223242526272829public class Test implements Externalizable &#123; private transient String content = &quot;我将会被序列化，不管我是否被transient关键字修饰&quot;; @Override public void writeExternal(ObjectOutput out) throws IOException &#123; out.writeObject(content); &#125; @Override public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException &#123; content = (String) in.readObject(); &#125; public static void main(String[] args) throws Exception &#123; Test test = new Test(); ObjectOutput out = new ObjectOutputStream(Files.newOutputStream(new File(&quot;test&quot;).toPath())); out.writeObject(test); ObjectInput in = new ObjectInputStream(Files.newInputStream(new File( &quot;test&quot;).toPath())); test = (Test) in.readObject(); System.out.println(test.content); out.close(); in.close(); &#125;&#125; 运行结果是: 1我将会被序列化，不管我是否被transient关键字修饰 ​ 这是为什么呢，不是说类的变量被transient关键字修饰以后将不能序列化了吗？ 我们知道在Java中，对象的序列化可以通过实现两种接口来实现，若实现的是Serializable接口，则所有的序列化将会自动进行，若实现的是Externalizable接口，则没有任何东西可以自动序列化，需要在writeExternal方法中进行手工指定所要序列化的变量，这与是否被transient修饰无关。因此第二个例子输出的是变量content初始化的内容，而不是null。","categories":[{"name":"Java","slug":"Java","permalink":"https://suavess.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://suavess.github.io/tags/Java/"},{"name":"基础","slug":"基础","permalink":"https://suavess.github.io/tags/%E5%9F%BA%E7%A1%80/"},{"name":"关键字","slug":"关键字","permalink":"https://suavess.github.io/tags/%E5%85%B3%E9%94%AE%E5%AD%97/"}]},{"title":"Java中switch关键字对String类型的支持","slug":"Java中switch关键字对String类型的支持","date":"2022-06-16T12:09:14.000Z","updated":"2022-06-16T12:45:42.845Z","comments":true,"path":"2022/06/16/Java中switch关键字对String类型的支持/","link":"","permalink":"https://suavess.github.io/2022/06/16/Java%E4%B8%ADswitch%E5%85%B3%E9%94%AE%E5%AD%97%E5%AF%B9String%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%94%AF%E6%8C%81/","excerpt":"","text":"在java7以后，switch的参数可以是String类型了，到目前为止switch支持这样几种数据类型：byte short int char String 。接下来我们就看一下，switch到底是如何实现的。 ps: 本文使用的反编译工具为jadx switch对整型的支持一段很简单的通过整型进行switch的代码 123456789101112131415public class Test &#123; public static void main(String[] args) &#123; int a = 1; switch (a) &#123; case 1: System.out.println(&quot;hello&quot;); break; case 2: System.out.println(&quot;world&quot;); break; default: break; &#125; &#125;&#125; 经过反编译后 12345678910111213141516package com.example.demo;/* loaded from: Test.class */public class Test &#123; public static void main(String[] args) &#123; switch (1) &#123; case 1: System.out.println(&quot;hello&quot;); return; case 2: System.out.println(&quot;world&quot;); return; default: return; &#125; &#125;&#125; 可以看到，经过反编译后的代码基本没有变化，因此switch对int的判断是直接比较整数的值。 switch对字符类型的支持123456789101112131415public class Test &#123; public static void main(String[] args) &#123; char a = &#x27;b&#x27;; switch (a) &#123; case &#x27;a&#x27;: System.out.println(&quot;hello&quot;); break; case &#x27;b&#x27;: System.out.println(&quot;world&quot;); break; default: break; &#125; &#125;&#125; 经过反编译之后 12345678910111213141516package com.example.demo;/* loaded from: Test.class */public class Test &#123; public static void main(String[] args) &#123; switch (98) &#123; case 97: System.out.println(&quot;hello&quot;); return; case 98: System.out.println(&quot;world&quot;); return; default: return; &#125; &#125;&#125; 通过以上的代码作比较我们发现：对char类型进行比较的时候，实际上比较的是ascii码，编译器会把char型变量转换成对应的int型变量 switch对字符串支持的实现12345678910111213public static void main(String[] args) &#123; String str = &quot;world&quot;; switch (str) &#123; case &quot;hello&quot;: System.out.println(&quot;hello&quot;); break; case &quot;world&quot;: System.out.println(&quot;world&quot;); break; default: break; &#125;&#125; 经过反编译之后 12345678910111213141516171819202122232425262728293031package com.example.demo;/* loaded from: Test.class */public class Test &#123; public static void main(String[] args) &#123; char c = 65535; switch (&quot;world&quot;.hashCode()) &#123; case 99162322: if (&quot;world&quot;.equals(&quot;hello&quot;)) &#123; c = 0; break; &#125; break; case 113318802: if (&quot;world&quot;.equals(&quot;world&quot;)) &#123; c = 1; break; &#125; break; &#125; switch (c) &#123; case 0: System.out.println(&quot;hello&quot;); return; case 1: System.out.println(&quot;world&quot;); return; default: return; &#125; &#125;&#125; 原来字符串的switch是通过equals()和hashCode()方法来实现的。记住，switch中只能使用整型，比如byte。short，char(ackii码是整型)以及int。还好hashCode()方法返回的是int，而不是long。通过这个很容易记住hashCode返回的是int这个事实。仔细看下可以发现，进行switch的实际是哈希值，然后通过使用equals方法比较进行安全检查，这个检查是必要的，因为哈希可能会发生碰撞。因此它的性能是不如使用枚举进行switch或者使用纯整数常量，但这也不是很差。因为Java编译器只增加了一个equals方法，如果你比较的是字符串字面量的话会非常快，比如”abc” ==”abc”。如果你把hashCode()方法的调用也考虑进来了，那么还会再多一次的调用开销，因为字符串一旦创建了，它就会把哈希值缓存起来。因此如果这个switch语句是用在一个循环里的，比如逐项处理某个值，或者游戏引擎循环地渲染屏幕，这里hashCode()方法的调用开销其实不会很大。 总结一下我们可以发现，其实switch只支持一种数据类型，那就是整型，其他数据类型都是转换成整型之后再使用switch的。","categories":[{"name":"Java","slug":"Java","permalink":"https://suavess.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://suavess.github.io/tags/Java/"},{"name":"反编译","slug":"反编译","permalink":"https://suavess.github.io/tags/%E5%8F%8D%E7%BC%96%E8%AF%91/"}]},{"title":"Java编译器对'+'的重载","slug":"Java编译器对'+'的重载","date":"2022-06-16T02:03:21.000Z","updated":"2022-06-16T06:10:00.912Z","comments":true,"path":"2022/06/16/Java编译器对'+'的重载/","link":"","permalink":"https://suavess.github.io/2022/06/16/Java%E7%BC%96%E8%AF%91%E5%99%A8%E5%AF%B9'+'%E7%9A%84%E9%87%8D%E8%BD%BD/","excerpt":"","text":"Java中对字符串进行拼接一般有两种方式，通过Stirng.concat()方法进行拼接，或者直接使用+号拼接，一般来说，我们都会使用第二种方式。 有人把Java中使用+拼接字符串的功能理解为运算符重载。其实并不是，Java是不支持运算符重载的。这其实只是Java提供的一个语法糖。 运算符重载：在计算机程序设计中，运算符重载（英语：operator overloading）是多态的一种。运算符重载，就是对已有的运算符重新进行定义，赋予其另一种功能，以适应不同的数据类型。 语法糖：语法糖（Syntactic sugar），也译为糖衣语法，是由英国计算机科学家彼得·兰丁发明的一个术语，指计算机语言中添加的某种语法，这种语法对语言的功能没有影响，但是更方便程序员使用。语法糖让程序更加简洁，有更高的可读性。 这样的一段代码，让我们看看反编译后的结果 使用IDEA直接查看编译后的class文件 通过查看反编译以后的代码，我们可以发现，原来字符串常量在拼接过程中，是将String转成了StringBuilder后，使用其append方法进行处理的。 那么也就是说，Java中的+对字符串的拼接，其实现原理是使用StringBuilder.append。 但是，String的使用+字符串拼接也不全都是基于StringBuilder.append，还有种特殊情况，那就是如果是两个固定的字面量拼接，如： 1String ab = &quot;a&quot; + &quot;b&quot;; 反编译后 这主要是因为两个字符串都是编译期常量，编译期可知，因此编译器会进行常量折叠，直接变成 String s = “ab”。","categories":[{"name":"Java","slug":"Java","permalink":"https://suavess.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://suavess.github.io/tags/Java/"},{"name":"反编译","slug":"反编译","permalink":"https://suavess.github.io/tags/%E5%8F%8D%E7%BC%96%E8%AF%91/"}]},{"title":"MySQL锁相关（二）","slug":"MySQL锁相关（二）","date":"2022-02-28T02:57:32.000Z","updated":"2022-06-16T02:01:19.365Z","comments":true,"path":"2022/02/28/MySQL锁相关（二）/","link":"","permalink":"https://suavess.github.io/2022/02/28/MySQL%E9%94%81%E7%9B%B8%E5%85%B3%EF%BC%88%E4%BA%8C%EF%BC%89/","excerpt":"","text":"Innodb行锁 行锁特点 ：偏向InnoDB 存储引擎，开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低,并发度也最高。 InnoDB 与 MyISAM 的最大不同有两点：一是支持事务；二是 采用了行级锁。 背景事务及其ACID属性事务是由一组SQL语句组成的逻辑处理单元。 事务具有以下4个特性，简称为事务ACID属性。 ACID属性 含义 原子性（Atomicity） 事务是一个原子操作单元，其对数据的修改，要么全部成功，要么全部失败。 一致性（Consistent） 在事务开始和完成时，数据都必须保持一致状态。 隔离性（Isolation） 数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的 “独立” 环境下运行。 持久性（Durable） 事务完成之后，对于数据的修改是永久的。 并发事务处理带来的问题 问题 含义 丢失更新（Lost Update） 当两个或多个事务选择同一行，最初的事务修改的值，会被后面的事务修改的值覆盖。 脏读（Dirty Reads） 当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。 不可重复读（Non-Repeatable Reads） 一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现和以前读出的数据不一致。 幻读（Phantom Reads） 一个事务按照相同的查询条件重新读取以前查询过的数据，却发现其他事务插入了满足其查询条件的新数据。 事务隔离级别为了解决上述提到的事务并发问题，数据库提供一定的事务隔离机制来解决这个问题。数据库的事务隔离越严格，并发副作用越小，但付出的代价也就越大，因为事务隔离实质上就是使用事务在一定程度上“串行化” 进行，这显然与“并发” 是矛盾的。 数据库的隔离级别有4个，由低到高依次为Read uncommitted、Read committed、Repeatable read、Serializable，这四个级别可以逐个解决脏写、脏读、不可重复读、幻读这几类问题。 隔离级别 丢失更新 脏读 不可重复读 幻读 Read uncommitted × √ √ √ Read committed × × √ √ Repeatable read（默认） × × × √ Serializable × × × × PS ： √ 代表可能出现 ， × 代表不会出现 。 Mysql 的数据库的默认隔离级别为 Repeatable read ， 查看方式： 12345-- MySQL8以前show variables like &#x27;tx_isolation&#x27;;-- MySQL8以后select @@global.transaction_isolation,@@transaction_isolation; InnoDB 的行锁模式InnoDB 实现了以下两种类型的行锁。 共享锁（S）：又称为读锁，简称S锁，共享锁就是多个事务对于同一数据可以共享一把锁，都能访问到数据，但是只能读不能修改。 排他锁（X）：又称为写锁，简称X锁，排他锁就是不能与其他锁并存，如一个事务获取了一个数据行的排他锁，其他事务就不能再获取该行的其他锁，包括共享锁和排他锁，但是获取排他锁的事务是可以对数据就行读取和修改。 对于UPDATE、DELETE和INSERT语句，InnoDB会自动给涉及数据集加排他锁（X)； 对于普通SELECT语句，InnoDB不会加任何锁； 可以通过以下语句显示给记录集加共享锁或排他锁 。 123共享锁（S）：SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE排他锁（X) ：SELECT * FROM table_name WHERE ... FOR UPDATE （悲观锁） 悲观锁和乐观锁 悲观锁：事务必须排队执行。数据锁住了，不允许并发。（行级锁：select后面添加for update） 乐观锁：支持并发，事务也不需要排队，只不过需要一个版本号。 行锁测试 Session-1 Session-2 关闭自动提交功能 关闭自动提交功能 可以正常的查询出全部的数据 可以正常的查询出全部的数据 查询id 为3的数据 ； 获取id为3的数据 ； 更新id为3的数据，但是不提交； 新id为3 的数据， 出于等待状态 通过commit， 提交事务 解除阻塞，更新正常进行 以上， 操作的都是同一行的数据，接下来，演示不同行的数据 ： 新id为3数据，正常的获取到行锁 ， 执行更新 ； 由于与Session-1 操作不是同一行，获取当前行锁，执行更新； 如果执行了更新语句，会对这一行数据加上排他锁（写锁），提交commit之后，会释放锁。另外一个线程update语句才可以执行解除阻塞状态。前提是两个线程操作同一行数据。 无索引时行锁升级为表锁 如果不通过索引条件检索数据，那么InnoDB将对表中的所有记录加锁，实际效果跟表锁一样。 查看当前表的索引 ： show index from test_innodb_lock\\G;（加上 \\G 表示将查询结果进行按列打印，可以使每个字段打印到单独的行） Session-1 Session-2 关闭自动提交功能 关闭自动提交功能 执行更新语句 ： 执行更新语句， 但处于阻塞状态： 提交事务： 解除阻塞，执行更新成功 ： 由于 执行更新时 ， name字段本来为varchar类型， 我们是作为数组类型使用，存在类型转换，索引失效，最终行锁变为表锁 ；(字符串类型，在SQL语句使用的时候没有加单引号，导致索引失效，查询没有走索引，进行全表扫描是，索引失效，行锁就升级为表锁) 间隙锁危害 当我们用范围条件，而不是使用相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据进行加锁； 对于键值在条件范围内但并不存在的记录，叫做 “间隙（GAP）” ， InnoDB也会对这个 “间隙” 加锁，这种锁机制就是所谓的 间隙锁（Next-Key锁） 。 示例 ： Session-1 Session-2 关闭自动提交功能 关闭自动提交功能 根据id范围更新数据 插入id为2的记录， 出于阻塞状态 提交事务 由于表数据中不存在id=2的数据，但是id&lt;4的行被加了排他锁，此时，这行数据就被加了间隙锁。无法插入 解除阻塞 ， 执行插入操作 ： 怎样避免间隙锁呢？ 在更新的时候，或者对数据行进行加锁的时候，尽量去缩小条件，使得间隙数据尽量的少，最大程度避免间隙锁的存在。 InnoDB 行锁争用情况1show status like &#x27;innodb_row_lock%&#x27;; Innodb_row_lock_current_waits: 当前正在等待锁定的数量 Innodb_row_lock_time: 从系统启动到现在锁定总时间长度 Innodb_row_lock_time_avg:每次等待所花平均时长 Innodb_row_lock_time_max:从系统启动到现在等待最长的一次所花的时间 Innodb_row_lock_waits: 系统启动后到现在总共等待的次数 当等待的次数很高，而且每次等待的时长也不小的时候，我们就需要分析系统中为什么会有如此多的等待，然后根据分析结果着手制定优化计划。 总结 InnoDB存储引擎由于实现了行级锁定，虽然在锁定机制的实现方面带来了性能损耗可能比表锁会更高一些，但是在整体并发处理能力方面要远远优于MyISAM的表锁的。当系统并发量较高的时候，InnoDB的整体性能和MyISAM相比就会有比较明显的优势。 但是，InnoDB的行级锁同样也有其脆弱的一面，当我们使用不当的时候，可能会让InnoDB的整体性能表现不仅不能比MyISAM高，甚至可能会更差。 优化建议： 尽可能让所有数据检索都能通过索引来完成，避免无索引行锁升级为表锁。 合理设计索引，尽量缩小锁的范围 尽可能减少索引条件，及索引范围，避免间隙锁 尽量控制事务大小，减少锁定资源量和时间长度 尽可使用低级别事务隔离（但是需要业务层面满足需求）","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://suavess.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://suavess.github.io/tags/MySQL/"},{"name":"行锁","slug":"行锁","permalink":"https://suavess.github.io/tags/%E8%A1%8C%E9%94%81/"}]},{"title":"MySQL锁相关（一）","slug":"MySQL锁相关（一）","date":"2022-02-27T08:07:46.000Z","updated":"2022-06-16T02:01:19.365Z","comments":true,"path":"2022/02/27/MySQL锁相关（一）/","link":"","permalink":"https://suavess.github.io/2022/02/27/MySQL%E9%94%81%E7%9B%B8%E5%85%B3%EF%BC%88%E4%B8%80%EF%BC%89/","excerpt":"","text":"锁分类从对数据操作的粒度分 ： 1） 表锁：操作时，会锁定整个表。 2） 行锁：操作时，会锁定当前操作行。 从对数据操作的类型分： 1） 读锁（共享锁）：针对同一份数据，多个读操作可以同时进行而不会互相影响。 2） 写锁（排它锁）：当前操作没有完成之前，它会阻断其他写锁和读锁。 MySQL中的锁相对其他数据库而言，MySQL的锁机制比较简单，其最显著的特点是不同的存储引擎支持不同的锁机制。下表中罗列出了各存储引擎对锁的支持情况： 存储引擎 表级锁 行级锁 页面锁 MyISAM 支持 不支持 不支持 InnoDB 支持 支持(默认) 不支持 MEMORY 支持 不支持 不支持 BDB 支持 不支持 支持 MySQL这3种锁的特性可大致归纳如下 ： 锁类型 特点 表级锁 偏向MyISAM 存储引擎，开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高,并发度最低。 行级锁 偏向InnoDB 存储引擎，开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低,并发度也最高。 页面锁 开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。 从上述特点可见，很难笼统地说哪种锁更好，只能就具体应用的特点来说哪种锁更合适！仅从锁的角度来说：表级锁更适合于以查询为主，只有少量按索引条件更新数据的应用，如Web 应用；而行级锁则更适合于有大量按索引条件并发更新少量不同数据，同时又有并查询的应用，如一些在线事务处理系统。 MyISAM的表锁MyISAM 存储引擎只支持表锁，这也是MySQL开始几个版本中唯一支持的锁类型。 如何加表锁 MyISAM 在执行查询语句（SELECT）前，会自动给涉及的所有表加读锁，在执行更新操作（UPDATE、DELETE、INSERT 等）前，会自动给涉及的表加写锁，这个过程并不需要用户干预，因此，用户一般不需要直接用 LOCK TABLE 命令给 MyISAM 表显式加锁。 显示加表锁语法： 123加读锁 ： lock table table_name read;加写锁 ： lock table table_name write； 读锁Session-1 ： 1）获得tb_book 表的读锁 1lock table tb_book read; 2） 执行查询操作 1select * from tb_book; 可以正常执行 ， 查询出数据。 Session-2： 3） 执行查询操作 1select * from tb_book; 也可以正常执行，查询出数据。 Session-1： 4）查询未锁定的表 1select name from tb_seller; Session-1查询未锁定的表失败。因为持有了一张tb_book的读锁，并未释放锁。 Session-2： 5）查询未锁定的表 1select name from tb_seller; 可以正常查询出未锁定的表； Session-1 ： 6） 执行插入操作 1insert into tb_book values(null,&#x27;Mysql高级&#x27;,&#x27;2088-01-01&#x27;,&#x27;1&#x27;); 执行插入， 直接报错 ， 由于当前tb_book 获得的是 读锁， 不能执行更新操作。只能读 Session-2 ： 7） 执行插入操作 1insert into tb_book values(null,&#x27;Mysql高级&#x27;,&#x27;2088-01-01&#x27;,&#x27;1&#x27;); 此时会处于等待状态，当在Session-1中释放锁指令 unlock tables 后 ， Session-2中的 inesrt 语句 ， 立即执行 ； 如果对某一张表加了读锁，不会阻塞其它线程的读操作，但是会阻塞其它线程的写操作。 写锁Session-1 : 1）获得tb_book 表的写锁 1lock table tb_book write ; 2）执行查询操作 1select * from tb_book ; 查询操作执行成功；加了写锁可以读。 3）执行更新操作 1update tb_book set name = &#x27;java编程思想（第二版）&#x27; where id = 1; 更新操作执行成功 ；（加了写锁当然可以写） Session-2 : 4）执行查询操作 1select * from tb_book ; 当在Session-1中释放锁指令 unlock tables 后 ， Session-2中的 select 语句 ， 立即执行 ；（因为Session-1线程加的是写锁，写锁是排他锁，会阻断其他线程的读和写操作） 结论锁模式的相互兼容性如表中所示： 由上表可见： 1） 对MyISAM 表的读操作，不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求； 2） 对MyISAM 表的写操作，则会阻塞其他用户对同一表的读和写操作； 简而言之，就是读锁会阻塞写，但是不会阻塞读。而写锁，则既会阻塞读，又会阻塞写。 此外，MyISAM 的读写锁调度是写优先，这也是MyISAM不适合做写为主的表的存储引擎的原因。因为写锁后，其他线程不能做任何操作，大量的更新会使查询很难得到锁，从而造成永远阻塞。 查看锁的争用情况1show open tables； In_user : 表当前被查询使用的次数。如果该数为零，则表是打开的，但是当前没有被使用。 Name_locked：表名称是否被锁定。名称锁定用于取消表或对表进行重命名等操作。 1show status like &#x27;Table_locks%&#x27;; Table_locks_immediate ： 指的是能够立即获得表级锁的次数，每立即获取锁，值加1。 Table_locks_waited ： 指的是不能立即获取表级锁而需要等待的次数，每等待一次，该值加1，此值高说明存在着较为严重的表级锁争用情况。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://suavess.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://suavess.github.io/tags/MySQL/"},{"name":"表锁","slug":"表锁","permalink":"https://suavess.github.io/tags/%E8%A1%A8%E9%94%81/"}]},{"title":"MySQL索引的失效场景","slug":"MySQL索引的失效场景","date":"2022-02-25T02:23:52.000Z","updated":"2022-06-16T02:01:19.365Z","comments":true,"path":"2022/02/25/MySQL索引的失效场景/","link":"","permalink":"https://suavess.github.io/2022/02/25/MySQL%E7%B4%A2%E5%BC%95%E7%9A%84%E5%A4%B1%E6%95%88%E5%9C%BA%E6%99%AF/","excerpt":"","text":"索引设置 1. 最左前缀法则 最左前缀法则指的是查询从索引的最左前列开始，并且不跳过索引中的列。 匹配最左前缀法则，where条件中的顺序不影响使用索引，MySQL优化器会自动选择 违反最左前缀法则，此时不走索引 符合最左索引，但中间有跳跃，此时只会走最左边的部分索引，可以看到key_len（即使用的索引长度）是比第一张图中的key_len小的 2. 范围查询右边的列 都走索引时，key_len字段为456 第二个条件使用范围搜索时，此时key_len是304，与只使用两个字段查询的key_len相同，说明后面的data_id字段没有走索引 3.在索引字段上进行运算操作 4.字段类型隐式转换 由于corpid字段是varchar类型，当where条件中不加引号时，mysql会进行隐式转换，此时索引会失效 5.用or分割的字段条件，其中有一个字段未建立索引 corpid为索引字段，而del为非索引字段 6. MySQL优化器判断索引查询更慢 当表中数据较少，或者查询的条件占表中数据的极大部分时，此时索引效率可能不如全表扫描，MySQL会选择全表扫描 7.以%开头的模糊查询 如果只是尾部的模糊查询是不会失效的 下图中使用强制索引是为了演示，因为表中数据较少，mysql判断索引速度不如全表扫描所以默认走了全表扫描 可以使用覆盖索引来解决该问题，即查询的列可以在索引中找到","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://suavess.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://suavess.github.io/tags/MySQL/"},{"name":"索引失效","slug":"索引失效","permalink":"https://suavess.github.io/tags/%E7%B4%A2%E5%BC%95%E5%A4%B1%E6%95%88/"}]},{"title":"MySQL中的触发器","slug":"MySQL中的触发器","date":"2022-02-24T11:36:53.000Z","updated":"2022-06-16T02:01:19.365Z","comments":true,"path":"2022/02/24/MySQL中的触发器/","link":"","permalink":"https://suavess.github.io/2022/02/24/MySQL%E4%B8%AD%E7%9A%84%E8%A7%A6%E5%8F%91%E5%99%A8/","excerpt":"","text":"1. 介绍 触发器是与表有关的数据库对象，指在 insert/update/delete 之前或之后，触发并执行触发器中定义的SQL语句集合。触发器的这种特性可以协助应用在数据库端确保数据的完整性 , 日志记录 , 数据校验等操作 。 使用别名 OLD 和 NEW 来引用触发器中发生变化的记录内容，这与其他的数据库是相似的。现在触发器还只支持行级触发，不支持语句级触发。（Oracle既有行级触发器，又有语句级触发器） 触发器类型 NEW 和 OLD的使用 INSERT 型触发器 NEW 表示将要或者已经新增的数据 UPDATE 型触发器 OLD 表示修改之前的数据 , NEW 表示将要或已经修改后的数据 DELETE 型触发器 OLD 表示将要或者已经删除的数据 2. 创建触发器2.1 语法结构 :1CREATE TRIGGER trigger_name trigger_time trigger_event ON tb_name FOR EACH ROW trigger_stmt tb_name ：需要建立触发器的表名(只能是永久表，不能对临时表创建触发器) trigger_name ：触发器名称，自行指定 trigger_time：触发时机，取值BEFORE、AFTER trigger_event ：触发事件，INSERT、UPDATE、DELETE trigger_stmt ： 触发程序体，可以是一条SQL语句或是BEGIN和END包含的多条语句 2.2 示例2.2.1 需求1通过触发器记录 emp 表的数据变更日志 , 包含增加, 修改 , 删除 ; 2.2.2 首先创建一张日志表 :12345678create table emp_logs( id int(11) not null auto_increment, operation varchar(20) not null comment &#x27;操作类型, insert/update/delete&#x27;, operate_time datetime not null comment &#x27;操作时间&#x27;, operate_id int(11) not null comment &#x27;操作表的ID&#x27;, operate_params varchar(500) comment &#x27;操作参数&#x27;, primary key(`id`))engine=innodb default charset=utf8; 2.2.3 创建 insert 型触发器，完成插入数据时的日志记录 :1234567891011DELIMITER $create trigger emp_logs_insert_triggerafter insert on emp for each row begin insert into emp_logs (id,operation,operate_time,operate_id,operate_params) values(null,&#x27;insert&#x27;,now(),new.id,concat(&#x27;插入后(id:&#x27;,new.id,&#x27;, name:&#x27;,new.name,&#x27;, age:&#x27;,new.age,&#x27;, salary:&#x27;,new.salary,&#x27;)&#x27;)); end $DELIMITER ; 2.2.4 创建 update 型触发器，完成更新数据时的日志记录 :1234567891011DELIMITER $create trigger emp_logs_update_triggerafter update on emp for each row begin insert into emp_logs (id,operation,operate_time,operate_id,operate_params) values(null,&#x27;update&#x27;,now(),new.id,concat(&#x27;修改前(id:&#x27;,old.id,&#x27;, name:&#x27;,old.name,&#x27;, age:&#x27;,old.age,&#x27;, salary:&#x27;,old.salary,&#x27;) , 修改后(id&#x27;,new.id, &#x27;name:&#x27;,new.name,&#x27;, age:&#x27;,new.age,&#x27;, salary:&#x27;,new.salary,&#x27;)&#x27;)); end $DELIMITER ; 2.2.5 创建delete 行的触发器 , 完成删除数据时的日志记录 :1234567891011DELIMITER $create trigger emp_logs_delete_triggerafter delete on emp for each row begin insert into emp_logs (id,operation,operate_time,operate_id,operate_params) values(null,&#x27;delete&#x27;,now(),old.id,concat(&#x27;删除前(id:&#x27;,old.id,&#x27;, name:&#x27;,old.name,&#x27;, age:&#x27;,old.age,&#x27;, salary:&#x27;,old.salary,&#x27;)&#x27;)); end $DELIMITER ; 2.2.6 测试：123456insert into emp(id,name,age,salary) values(null, &#x27;光明左使&#x27;,30,3500);insert into emp(id,name,age,salary) values(null, &#x27;光明右使&#x27;,33,3200);update emp set age = 39 where id = 3;delete from emp where id = 5; 3. 删除触发器语法结构 : 1drop trigger [schema_name.]trigger_name 如果没有指定 schema_name，默认为当前数据库 。 4. 查看触发器可以通过执行 SHOW TRIGGERS 命令查看触发器的状态、语法等信息。 语法结构 ： 1show triggers ; 5. 总结 优点是可以在数据库层面保证数据的完整性，并且可以少写业务逻辑代码，使用方便。例如记录日志的功能，可以节省极大量的逻辑代码 缺点也很明显，逻辑在数据库层面，应用层调试困难，出现问题难以定位 注意点 如果BEFORE触发器执行失败，SQL无法正确执行。 SQL执行失败时，AFTER型触发器不会触发。 AFTER类型的触发器执行失败，SQL会回滚。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://suavess.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://suavess.github.io/tags/MySQL/"},{"name":"触发器","slug":"触发器","permalink":"https://suavess.github.io/tags/%E8%A7%A6%E5%8F%91%E5%99%A8/"}]},{"title":"MySQL索引结构","slug":"MySQL索引结构","date":"2022-02-22T02:38:57.000Z","updated":"2022-06-16T02:01:19.365Z","comments":true,"path":"2022/02/22/MySQL索引结构/","link":"","permalink":"https://suavess.github.io/2022/02/22/MySQL%E7%B4%A2%E5%BC%95%E7%BB%93%E6%9E%84/","excerpt":"","text":"索引是在MySQL的存储引擎层中实现的，而不是在服务器层实现的。所以每种存储引擎的索引都不一定完全相同，也不是所有的存储引擎都支持所有的索引类型的。MySQL目前提供了以下4种索引： BTREE 索引 ： 最常见的索引类型，大部分索引都支持 B 树索引。 HASH 索引：只有Memory引擎支持 ， 使用场景简单 。 R-tree 索引（空间索引）：空间索引是MyISAM引擎的一个特殊索引类型，主要用于地理空间数据类型，通常使用较少，不做特别介绍。 Full-text （全文索引） ：全文索引也是MyISAM的一个特殊索引类型，主要用于全文索引，InnoDB从Mysql5.6版本开始支持全文索引。 MyISAM、InnoDB、Memory三种存储引擎对各种索引类型的支持 索引 InnoDB引擎 MyISAM引擎 Memory引擎 BTREE索引 支持 支持 支持 HASH 索引 不支持 不支持 支持 R-tree 索引 不支持 支持 不支持 Full-text 5.6版本之后支持 支持 不支持 我们平常所说的索引，如果没有特别指明，都是指B+树（多路搜索树，并不一定是二叉的）结构组织的索引。其中聚集索引、复合索引、前缀索引、唯一索引默认都是使用 B+tree 索引，统称为 索引。 BTree索引BTree又叫多路平衡搜索树，一颗m叉的BTree特性如下： 树中每个节点最多包含m个孩子。 除根节点与叶子节点外，每个节点至少有[(m/2)]个孩子。 若根节点不是叶子节点，则至少有两个孩子。 所有的叶子节点都在同一层。 每个非叶子节点由n个key与n+1个指针组成，其中[ceil(m/2)-1] &lt;= n &lt;= m-1 以5叉BTree为例，key的数量：公式推导[ceil(m/2)-1] &lt;= n &lt;= m-1。所以 2 &lt;= n &lt;=4 。当n&gt;4时，中间节点分裂到父节点，两边节点分裂。 插入 C N G A H E K Q M F W L T Z D P R X Y S 数据为例。 演变过程如下： 1). 插入前4个字母 C N G A 2). 插入H，n&gt;4，中间元素G字母向上分裂到新的节点 3). 插入E，K，Q不需要分裂 4). 插入M，中间元素M字母向上分裂到父节点G 5). 插入F，W，L，T不需要分裂 6). 插入Z，中间元素T向上分裂到父节点中 7). 插入D，中间元素D向上分裂到父节点中。然后插入P，R，X，Y不需要分裂 8). 最后插入S，NPQR节点n&gt;5，中间节点Q向上分裂，但分裂后父节点DGMT的n&gt;5，中间节点M向上分裂 到此，该BTREE树就已经构建完成了， BTREE树 和 二叉树 相比， 查询数据的效率更高， 因为对于相同的数据量来说，BTREE的层级结构比二叉树小，因此搜索速度快。 B+TREE 结构B+Tree为BTree的变种，B+Tree与BTree的区别为： 1). n叉B+Tree最多含有n个key，而BTree最多含有n-1个key。 2). B+Tree的叶子节点保存所有的key信息，依key大小顺序排列。 3). 所有的非叶子节点都可以看作是key的索引部分。 由于B+Tree只有叶子节点保存key信息，查询任何key都要从root走到叶子。所以B+Tree的查询效率更加稳定。 MySQL中的B+TreeMySql索引数据结构对经典的B+Tree进行了优化。在原B+Tree的基础上，增加一个指向相邻叶子节点的链表指针，就形成了带有顺序指针的B+Tree，提高区间访问的性能。 MySQL中的 B+Tree 索引结构示意图:","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://suavess.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://suavess.github.io/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"https://suavess.github.io/tags/%E7%B4%A2%E5%BC%95/"},{"name":"数据结构","slug":"数据结构","permalink":"https://suavess.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"8、ElasticSearch索引Mapping映射关系","slug":"8、ElasticSearch索引Mapping映射关系","date":"2021-10-28T02:08:35.000Z","updated":"2022-06-16T02:01:19.364Z","comments":true,"path":"2021/10/28/8、ElasticSearch索引Mapping映射关系/","link":"","permalink":"https://suavess.github.io/2021/10/28/8%E3%80%81ElasticSearch%E7%B4%A2%E5%BC%95Mapping%E6%98%A0%E5%B0%84%E5%85%B3%E7%B3%BB/","excerpt":"","text":"ElasticSearch中的映射关系类似于关系型数据库中的表结构，用于说明该字段的类型及约束条件 创建一个新的索引1PUT user 创建映射1234567891011121314151617PUT user/_mapping&#123; &quot;properties&quot;: &#123; &quot;name&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;index&quot;: true &#125;, &quot;sex&quot;:&#123; &quot;type&quot;: &quot;keyword&quot;, &quot;index&quot;: true &#125;, &quot;tel&quot;:&#123; &quot;type&quot;: &quot;keyword&quot;, &quot;index&quot;: false &#125; &#125;&#125; 查询映射1GET user/_mapping 映射作用 先插入一条数据 123456PUT user/_create/1001&#123; &quot;name&quot;:&quot;小米&quot;, &quot;sex&quot;:&quot;男的&quot;, &quot;tel&quot;:&quot;1111&quot;&#125; 查询name中含有“小”的数据 12345678GET user/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;name&quot;:&quot;小&quot; &#125; &#125;&#125; 查询sex含有”男“的数据 12345678GET user/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;sex&quot;:&quot;男&quot; &#125; &#125;&#125; 可以看到没有返回数据，原因是sex这个字段指定了类型为keyword，而keyword类型的字段不会被分词，因此需要完全匹配上才可以查询出数据 查询sex含有”男的“的数据 12345678GET user/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;sex&quot;:&quot;男的&quot; &#125; &#125;&#125; 查询电话 12345678GET user/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;tel&quot;:&quot;11&quot; &#125; &#125;&#125; 此时会报错，原因是创建映射时，指定了index为false，所以无法通过这个字段查询数据","categories":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://suavess.github.io/categories/ElasticSearch/"}],"tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://suavess.github.io/tags/ElasticSearch/"}]},{"title":"7、ElasticSearch文档的查询(3)","slug":"7、ElasticSearch文档的查询(3)","date":"2021-10-27T08:56:19.000Z","updated":"2022-06-16T02:01:19.364Z","comments":true,"path":"2021/10/27/7、ElasticSearch文档的查询(3)/","link":"","permalink":"https://suavess.github.io/2021/10/27/7%E3%80%81ElasticSearch%E6%96%87%E6%A1%A3%E7%9A%84%E6%9F%A5%E8%AF%A2(3)/","excerpt":"","text":"高亮查询12345678910111213GET shopping/_search&#123; &quot;query&quot;:&#123; &quot;match_phrase&quot;:&#123; &quot;category&quot; : &quot;为&quot; &#125; &#125;, &quot;highlight&quot;:&#123; &quot;fields&quot;:&#123; &quot;category&quot;:&#123;&#125; &#125; &#125;&#125; 聚合查询聚合查询类似关系型数据库中的 group by，当然还有很多其他的聚合，例如取最大值max、平均值avg等等。 按price字段进行分组： 12345678910GET shopping/_search&#123; &quot;aggs&quot;:&#123; &quot;price_group&quot;:&#123; &quot;terms&quot;:&#123; &quot;field&quot;:&quot;price&quot; &#125; &#125; &#125;&#125; 上述的返回结果会携带原始数据，如果不想携带可以新增一个size的字段 1234567891011GET shopping/_search&#123; &quot;aggs&quot;:&#123; &quot;price_group&quot;:&#123; &quot;terms&quot;:&#123; &quot;field&quot;:&quot;price&quot; &#125; &#125; &#125;, &quot;size&quot;: 0&#125; 对所有手机价格求平均值 1234567891011GET shopping/_search&#123; &quot;aggs&quot;:&#123; &quot;price_avg&quot;:&#123; &quot;avg&quot;:&#123; &quot;field&quot;:&quot;price&quot; &#125; &#125; &#125;, &quot;size&quot;:0&#125;","categories":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://suavess.github.io/categories/ElasticSearch/"}],"tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://suavess.github.io/tags/ElasticSearch/"}]},{"title":"6、ElasticSearch文档的查询(2)","slug":"6、ElasticSearch文档的查询(2)","date":"2021-10-27T08:14:19.000Z","updated":"2022-06-16T02:01:19.364Z","comments":true,"path":"2021/10/27/6、ElasticSearch文档的查询(2)/","link":"","permalink":"https://suavess.github.io/2021/10/27/6%E3%80%81ElasticSearch%E6%96%87%E6%A1%A3%E7%9A%84%E6%9F%A5%E8%AF%A2(2)/","excerpt":"","text":"多条件查询 假设想找出小米牌子，价格为3999元的。（must相当于数据库的&amp;&amp;） 12345678910111213141516GET shopping/_search&#123; &quot;query&quot;:&#123; &quot;bool&quot;:&#123; &quot;must&quot;:[&#123; &quot;match&quot;:&#123; &quot;category&quot;:&quot;小米&quot; &#125; &#125;,&#123; &quot;match&quot;:&#123; &quot;price&quot;:3999.00 &#125; &#125;] &#125; &#125;&#125; 假设想找出小米和华为的牌子。（should相当于数据库的||） 12345678910111213141516GET shopping/_search&#123; &quot;query&quot;:&#123; &quot;bool&quot;:&#123; &quot;should&quot;:[&#123; &quot;match&quot;:&#123; &quot;category&quot;:&quot;小米&quot; &#125; &#125;,&#123; &quot;match&quot;:&#123; &quot;category&quot;:&quot;华为&quot; &#125; &#125;] &#125;, &#125;&#125; 假设想找出小米和华为的牌子，价格大于4000元的手机。 1234567891011121314151617181920212223242526GET shopping/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;category&quot;: &quot;小米&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;category&quot;: &quot;华为&quot; &#125; &#125; ], &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;price&quot;: &#123; &quot;gt&quot;: 4000 &#125; &#125; &#125; &#125; &#125;&#125; 完全匹配查询即需要完全包含搜索条件的查询，查询条件不做分词处理 12345678GET shopping/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;category&quot;: &quot;小为&quot; &#125; &#125;&#125;","categories":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://suavess.github.io/categories/ElasticSearch/"}],"tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://suavess.github.io/tags/ElasticSearch/"}]},{"title":"5、ElasticSearch文档的查询(1)","slug":"5、ElasticSearch文档的查询(1)","date":"2021-10-27T07:28:09.000Z","updated":"2022-06-16T02:01:19.364Z","comments":true,"path":"2021/10/27/5、ElasticSearch文档的查询(1)/","link":"","permalink":"https://suavess.github.io/2021/10/27/5%E3%80%81ElasticSearch%E6%96%87%E6%A1%A3%E7%9A%84%E6%9F%A5%E8%AF%A2(1)/","excerpt":"","text":"主键查询1GET shopping/_doc/1 全查询1GET shopping/_search 条件查询通过链接上拼接参数的方式1GET shopping/_search?q=category:华为 PS:不建议将参数拼接在链接上，可能会有乱码问题 通过请求体12345678GET shopping/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;category&quot;:&quot;华为&quot; &#125; &#125;&#125; 通过请求体查询所有123456GET shopping/_search&#123; &quot;query&quot;:&#123; &quot;match_all&quot;:&#123;&#125; &#125;&#125; 查询指定字段1234567GET shopping/_search&#123; &quot;query&quot;:&#123; &quot;match_all&quot;:&#123;&#125; &#125;, &quot;_source&quot;:[&quot;title&quot;]&#125; 分页查询12345678GET shopping/_search&#123; &quot;query&quot;:&#123; &quot;match_all&quot;:&#123;&#125; &#125;, &quot;from&quot;:0, &quot;size&quot;:1&#125; 查询排序1234567891011GET shopping/_search&#123; &quot;query&quot;:&#123; &quot;match_all&quot;:&#123;&#125; &#125;, &quot;sort&quot;:&#123; &quot;price&quot;:&#123; &quot;order&quot;:&quot;desc&quot; &#125; &#125;&#125;","categories":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://suavess.github.io/categories/ElasticSearch/"}],"tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://suavess.github.io/tags/ElasticSearch/"}]},{"title":"4、ElasticSearch文档的修改","slug":"4、ElasticSearch文档的修改","date":"2021-10-27T05:56:43.000Z","updated":"2022-06-16T02:01:19.364Z","comments":true,"path":"2021/10/27/4、ElasticSearch文档的修改/","link":"","permalink":"https://suavess.github.io/2021/10/27/4%E3%80%81ElasticSearch%E6%96%87%E6%A1%A3%E7%9A%84%E4%BF%AE%E6%94%B9/","excerpt":"","text":"完全覆盖修改完全覆盖操作是幂等操作，因此可以使用PUT方法 1234567PUT shopping/_doc/1&#123; &quot;title&quot;:&quot;华为手机&quot;, &quot;category&quot;:&quot;华为&quot;, &quot;images&quot;:&quot;http://www.gulixueyuan.com/hw.jpg&quot;, &quot;price&quot;:1999.00&#125; 此时再获取一下 1GET shopping/_doc/1 可以看到整个数据体被完全覆盖了 局部修改123456POST shopping/_upodate/1&#123; &quot;doc&quot;: &#123; &quot;title&quot;:&quot;OPPO手机&quot; &#125;&#125; 此时再获取一遍","categories":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://suavess.github.io/categories/ElasticSearch/"}],"tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://suavess.github.io/tags/ElasticSearch/"}]},{"title":"3、ElasticSearch文档的创建及删除","slug":"3、ElasticSearch文档的创建","date":"2021-10-27T03:16:14.000Z","updated":"2022-06-16T02:01:19.363Z","comments":true,"path":"2021/10/27/3、ElasticSearch文档的创建/","link":"","permalink":"https://suavess.github.io/2021/10/27/3%E3%80%81ElasticSearch%E6%96%87%E6%A1%A3%E7%9A%84%E5%88%9B%E5%BB%BA/","excerpt":"","text":"创建文档创建文档需要使用POST请求，如果用PUT请求则会报错，原因是PUT请求应当为幂等性操作，而不指定id直接创建文档时，会不断生成新的文档，该操作不为幂等操作，因此无法使用PUT请求 1234567PUT shopping/_doc&#123; &quot;title&quot;:&quot;小米手机&quot;, &quot;category&quot;:&quot;小米&quot;, &quot;images&quot;:&quot;http://www.gulixueyuan.com/xm.jpg&quot;, &quot;price&quot;:3999.00&#125; 1234567POST shopping/_doc&#123; &quot;title&quot;:&quot;小米手机&quot;, &quot;category&quot;:&quot;小米&quot;, &quot;images&quot;:&quot;http://www.gulixueyuan.com/xm.jpg&quot;, &quot;price&quot;:3999.00&#125; PS:创建文档时，如果指定索引不存在，则会自动创建该索引 _id为ElasticSearch自动为该文档生成的id，如果想要自己指定id，则需要在url后面拼接上id 1234567POST shopping/_doc/1&#123; &quot;title&quot;:&quot;小米手机&quot;, &quot;category&quot;:&quot;小米&quot;, &quot;images&quot;:&quot;http://www.gulixueyuan.com/xm.jpg&quot;, &quot;price&quot;:3999.00&#125; 指定id后，操作就是幂等操作了，因此理论上可以换成PUT方式请求， 1234567PUT shopping/_doc/1&#123; &quot;title&quot;:&quot;小米手机&quot;, &quot;category&quot;:&quot;小米&quot;, &quot;images&quot;:&quot;http://www.gulixueyuan.com/xm.jpg&quot;, &quot;price&quot;:3999.00&#125; 删除文档同理，只要将请求方式改为DELETE即可 1DELETE shopping/_doc/1","categories":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://suavess.github.io/categories/ElasticSearch/"}],"tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://suavess.github.io/tags/ElasticSearch/"}]},{"title":"2、ElasticSearch索引的创建查询及删除","slug":"2、ElasticSearch索引的创建查询及删除","date":"2021-10-27T02:30:25.000Z","updated":"2022-06-16T02:01:19.363Z","comments":true,"path":"2021/10/27/2、ElasticSearch索引的创建查询及删除/","link":"","permalink":"https://suavess.github.io/2021/10/27/2%E3%80%81ElasticSearch%E7%B4%A2%E5%BC%95%E7%9A%84%E5%88%9B%E5%BB%BA%E6%9F%A5%E8%AF%A2%E5%8F%8A%E5%88%A0%E9%99%A4/","excerpt":"","text":"后续操作均在Kibana的Dev Tools中完成 创建索引创建使用PUT的请求方式 先创建一个名为shopping的索引 1PUT shopping 此时再次调用则会报错 查询索引 查询指定索引 将PUT改为GET即可 1GET shopping 查询所有的索引 加上?v后会展示字段头信息 1GET _cat/indices?v 删除索引与查询指定索引的逻辑一致，将请求方式改为DELETE即可 1DELETE shopping 此时再查询则会返回404 1GET shopping","categories":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://suavess.github.io/categories/ElasticSearch/"}],"tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://suavess.github.io/tags/ElasticSearch/"}]},{"title":"1、Docker安装ElasticSearch和Kibana","slug":"1、Docker安装ElasticSearch和Kibana","date":"2021-10-26T13:09:45.000Z","updated":"2022-06-16T02:01:19.363Z","comments":true,"path":"2021/10/26/1、Docker安装ElasticSearch和Kibana/","link":"","permalink":"https://suavess.github.io/2021/10/26/1%E3%80%81Docker%E5%AE%89%E8%A3%85ElasticSearch%E5%92%8CKibana/","excerpt":"","text":"安装ElasticSearch先拉下es的镜像 1docker pull elasticsearch:7.12.0 镜像拉取完毕后新建容器 1docker run --name elasticsearch -p 9200:9200 -d elasticsearch:7.12.0 等es启动完成后，可以在命令行输入 1curl http://127.0.0.1:9200 或者在浏览器中打开http://127.0.0.1:9200这个网址，如果能看到以下信息则说明我们的es是已经安装好了的。 1234567891011121314151617&#123; &quot;name&quot;: &quot;e0c0b0ff715b&quot;, &quot;cluster_name&quot;: &quot;elasticsearch&quot;, &quot;cluster_uuid&quot;: &quot;c42jGCPrRHiGTix4pUl9EQ&quot;, &quot;version&quot;: &#123; &quot;number&quot;: &quot;7.12.0&quot;, &quot;build_flavor&quot;: &quot;default&quot;, &quot;build_type&quot;: &quot;docker&quot;, &quot;build_hash&quot;: &quot;78722783c38caa25a70982b5b042074cde5d3b3a&quot;, &quot;build_date&quot;: &quot;2021-03-18T06:17:15.410153305Z&quot;, &quot;build_snapshot&quot;: false, &quot;lucene_version&quot;: &quot;8.8.0&quot;, &quot;minimum_wire_compatibility_version&quot;: &quot;6.8.0&quot;, &quot;minimum_index_compatibility_version&quot;: &quot;6.0.0-beta1&quot; &#125;, &quot;tagline&quot;: &quot;You Know, for Search&quot;&#125; 安装IK分词器es自带的分词器对中文分词不是很友好，所以我们下载开源的IK分词器来解决这个问题。首先进入容器，然后进入plugins目录中下载分词器，下载完成后重启es即可。具体步骤如下: 注意：elasticsearch的版本和ik分词器的版本需要保持一致，不然在重启的时候会失败。 123docker exec -it elasticsearch /bin/bashcd /usr/share/elasticsearch/plugins/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.12.0/elasticsearch-analysis-ik-7.12.0.zip 安装Kibana同样适用docker安装kibana命令如下: 1docker pull kibana:7.12.0 安装完成以后需要启动kibana容器，需要使用--link连接到elasticsearch容器，--link命令的格式为name:alias，即需要绑定的容器名称以及赋予的别名 1docker run --name kibana --link=elasticsearch:es -p 5601:5601 -d kibana:7.12.0 启动后需要修改kibana的配置文件，将连接es的地址修改为创建容器时填入的别名 123docker exec -it elasticsearch /bin/bashcd /usr/share/kibana/config/vi kibana.yml 将elasticsearch.hosts项修改为 [ &quot;http://es:9200&quot; ] 重启kibana后在浏览器中输入kibana的地址即可打开kibana的页面了","categories":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://suavess.github.io/categories/ElasticSearch/"}],"tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://suavess.github.io/tags/ElasticSearch/"},{"name":"Docker","slug":"Docker","permalink":"https://suavess.github.io/tags/Docker/"}]},{"title":"MySQL-limit数据丢失问题","slug":"Mysql-limit数据丢失问题","date":"2021-10-26T07:06:42.000Z","updated":"2022-06-16T02:01:19.366Z","comments":true,"path":"2021/10/26/Mysql-limit数据丢失问题/","link":"","permalink":"https://suavess.github.io/2021/10/26/Mysql-limit%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1%E9%97%AE%E9%A2%98/","excerpt":"","text":"今天在项目中遇到一个很奇怪的bug，前端调接口分页返回数据时，有两条数据在第一页的最后返回了，然后又在第二页的头返回了，本以为是程序的问题，排查了半天最后发现是Mysql的问题。 Mysql表数据 查询流程先尝试查询前二十条数据1SELECT * FROM tb_customer_open_sea WHERE corpid = &#x27;1&#x27; AND del = 0 LIMIT 0,20; 返回结果 可以看到id为2和3的数据丢失了，同时返回了id为21和22的数据，此时再查询第二页的数据时 1SELECT * FROM tb_customer_open_sea WHERE corpid = &#x27;1&#x27; AND del = 0 LIMIT 20,20; 再次返回了id为21和22的数据，这就离谱了，丢了两条数据，难道是没满足where条件吗？于是我去掉了limit试了试 id为2和3的数据出现了….而和刚才那条SQL的区别只是去掉了limit，难道limit还会影响查询结果？？？然后我又在limit的基础上加上了order by 此时返回的数据中包含了2和3，说明是满足条件的，只是返回的顺序不对可能2和3的数据排在了二十条以后，但是为什么第二页也不包含这两条数据呢 于是我explain了一下这两条sql，发现这两条sql唯一的区别就是第一条走了联合索引，第二条没走联合索引，于是我又尝试了让第二条sql走索引 消失的数据又出现了，果然是索引的问题(不用limit 20,9是因为表中一共就29条数据，这样的话就是全表扫描不会走索引了) 总结当使用limit不使用order by时，且查询条件走了普通索引，就可能会按普通索引的顺序返回数据，所以用了limit就尽量加上order by","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://suavess.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://suavess.github.io/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"https://suavess.github.io/tags/%E7%B4%A2%E5%BC%95/"}]},{"title":"Java并发-CAS-乐观锁-解析","slug":"Java并发-CAS-乐观锁-解析","date":"2021-05-26T10:05:24.000Z","updated":"2022-06-16T02:01:19.364Z","comments":true,"path":"2021/05/26/Java并发-CAS-乐观锁-解析/","link":"","permalink":"https://suavess.github.io/2021/05/26/Java%E5%B9%B6%E5%8F%91-CAS-%E4%B9%90%E8%A7%82%E9%94%81-%E8%A7%A3%E6%9E%90/","excerpt":"","text":"本文讲解CAS机制，主要是因为最近准备面试题，发现这个问题在面试中出现的频率非常的高，因此把自己学习过程中的一些理解记录下来。 乐观锁与悲观锁 synchronized是悲观锁，这种线程一旦得到锁，其他需要锁的线程就挂起的情况就是悲观锁。 CAS操作的就是乐观锁，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。 模拟线程安全问题先看下面的代码，执行结果不言而喻，最终的结果一定是小于预期的1000 123456789101112131415161718192021222324252627282930313233343536/** * 并发时线程安全问题 * * @author Suave * @author 2021/3/5 1:50 下午 */public class Test01 &#123; static int count = 0; public static void main(String[] args) throws InterruptedException &#123; long startTime = System.currentTimeMillis(); int threadSize = 100; CountDownLatch countDownLatch = new CountDownLatch(threadSize); for (int i = 0; i &lt; threadSize; i++) &#123; new Thread(() -&gt; &#123; for (int j = 0; j &lt; 10; j++) &#123; try &#123; request(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; countDownLatch.countDown(); &#125;).start(); &#125; countDownLatch.await(); long endTime = System.currentTimeMillis(); System.out.printf(&quot;消耗时长: %d, 结果: %d&quot;, endTime - startTime, count); &#125; public static void request() throws InterruptedException &#123; TimeUnit.MILLISECONDS.sleep(5); count++; &#125;&#125; 这段代码线程不安全的具体原因是 count++ 并不是一个原子性的操作，count++实际会经过三个步骤 获取 count 的值，先记为 A A = count 将 A 的值加一，得到B B = A+1 将 B 的值赋给 count count = B 在多线程的情况下，可能会有多个线程同时走到第一步，同时获取到了相同的A 此时新增后将值赋给B就会出现多个线程调用了这个方法结果count只加了一的情况 线程安全问题解决 线程安全的问题可以通过加锁的方式解决，先来试试最常用也最方便的synchronized关键字，即假设每次都会有冲突，一次只允许一个线程进入的悲观锁 12345// 在request方法上面加上synchronized关键字public synchronized static void request() throws InterruptedException &#123; TimeUnit.MILLISECONDS.sleep(5); count++;&#125; ​ 再来运行试一下 ​ 可以看到，执行结果和我们预期的一致，线程已经安全了。 ​ 但是，这个运行效率就很感人了……… 用CAS乐观锁解决 增加一个CAS方法，同时更改request方法，每次赋值的时候去比较一下值是否发生了改变，改变了就重试 123456789101112131415161718192021/** * CAS方法 * * @param expectCount 期望值count * @param newCount 需要给count赋的新值 * @return true 成功 false 失败 */ public static synchronized boolean compareAndSwap(int expectCount, int newCount) &#123; if (count == expectCount) &#123; count = newCount; return true; &#125; return false; &#125; public static void request() throws InterruptedException &#123; TimeUnit.MILLISECONDS.sleep(5); int expectCount; // expectCount返回false时要再获取count的值并重新赋值 while (!compareAndSwap(expectCount=count, expectCount + 1)) &#123;&#125; &#125; 可以看到，执行结果与我们预期的一致，性能也要强于synchronized悲观锁 那么这段乐观锁的代码就一定不会出问题了吗？ CPU执行某一段代码的时候，会先将数据读取到高速缓存中，极端情况下，当该线程将数据读取到高速缓存中时，其他线程更新了主存中的值，此时我们缓存中的值就不准确了 当我们以缓存中的值做CAS操作时，就会与预期值不一致了，因此，还要在 count 属性上加上 volatile 关键字，保证程序的可见性，每次获取该值时都去主存中获取 CAS的缺点当然，CAS也是有缺点的 CPU开销较大在并发量比较高的情况下，如果许多线程反复尝试更新某一个变量，却又一直更新不成功，循环往复，会给CPU带来很大的压力。 不能保证代码块的原子性CAS机制所保证的只是一个变量的原子性操作，而不能保证整个代码块的原子性。比如需要保证3个变量共同进行原子性的更新，就不得不使用Synchronized了。","categories":[{"name":"Java","slug":"Java","permalink":"https://suavess.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://suavess.github.io/tags/Java/"},{"name":"锁机制","slug":"锁机制","permalink":"https://suavess.github.io/tags/%E9%94%81%E6%9C%BA%E5%88%B6/"}]},{"title":"Java集合类-RandomAccess接口","slug":"Java集合类-RandomAccess接口","date":"2021-04-26T04:59:21.000Z","updated":"2022-06-16T02:01:19.365Z","comments":true,"path":"2021/04/26/Java集合类-RandomAccess接口/","link":"","permalink":"https://suavess.github.io/2021/04/26/Java%E9%9B%86%E5%90%88%E7%B1%BB-RandomAccess%E6%8E%A5%E5%8F%A3/","excerpt":"","text":"发现RandomAccess接口因为面试的时候问到了怎样使一个 ArrayList 变得线程安全 使用老牌的Vetor 使用CopyOnWriteArrayList 使用Collections.syncronizedList()方法传入一个List，会返回一个线程安全的List 答完后引申到Collections.syncronizedList()的实现原理，当时没看过具体实现并未回答上来。复盘时研究了下，发现首先会先判断传过来的List是否实现了RandomAccess接口 RandomAccess接口详解点进Random接口发现该接口并没有任何属性和任何方法 于是查看jdk8官方文档 官方文档写明了，RandomAccess接口主要是一个标记接口，用来注明这个List是否支持随机访问。 ArrayList实现了Random接口而LinkedList并未实现 12public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable &#123;&#125;public class LinkedList&lt;E&gt; extends AbstractSequentialList&lt;E&gt; implements List&lt;E&gt;, Deque&lt;E&gt;, Cloneable, java.io.Serializable &#123;&#125; ArrayList底层是数组，占用一片连续的空间，可以通过计算偏移量来直接获取元素，即支持随机访问，此时使用for (int i=0, n=list.size(); i &lt; n; i++){}循环时的速度要快于iterator顺序循环访问 LinkedList底层是双向链表，遍历要按顺序一个元素节点一个元素节点查询下去，此时iterator速度要快于for (int i=0, n=list.size(); i &lt; n; i++){} 性能测试 随机往数组和链表中插入100000个元素，并分别打印遍历时间 遍历结果 总结可以看到ArrayList（即RandomAccess接口标识的类）使用普通for循环遍历速度较快 而LinkedList（即未被RandomAccess接口标识的类）使用迭代器遍历速度远快于普通for遍历 后续遍历时可以根据是否被RandomAccess接口标识选择更优的遍历方式","categories":[{"name":"Java","slug":"Java","permalink":"https://suavess.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://suavess.github.io/tags/Java/"},{"name":"集合","slug":"集合","permalink":"https://suavess.github.io/tags/%E9%9B%86%E5%90%88/"}]},{"title":"Java中的值传递和引用传递","slug":"Java中的值传递和引用传递","date":"2020-07-26T07:06:05.000Z","updated":"2022-06-16T02:01:19.364Z","comments":true,"path":"2020/07/26/Java中的值传递和引用传递/","link":"","permalink":"https://suavess.github.io/2020/07/26/Java%E4%B8%AD%E7%9A%84%E5%80%BC%E4%BC%A0%E9%80%92%E5%92%8C%E5%BC%95%E7%94%A8%E4%BC%A0%E9%80%92/","excerpt":"","text":"Java中的参数传递分为值传递和引用传递，基本类型是值传递，封装类是引用传递。 值传递1234567891011public class Test01 &#123; public static void change(int num) &#123; num = 1; &#125; public static void main(String[] args) &#123; int num = 0; change(num); System.out.println(&quot;num=&quot; + num); &#125;&#125; 上面这段代码的输出结果是num=0。 当调用change方法时，jvm实际上是复制了一份参数传入方法中，无论这个复制后的参数怎么变，main方法中的num参数都是不会发生变化的。 从内存分配的角度来看，栈中是有两个变量的。一个是main方法中定义的num，另外一个是在传参时jvm复制的一份num副本，我们就叫它temp。不论temp值怎么改变，num是不会变的。 引用传值1234567891011121314151617181920212223242526public class Test01 &#123; public static void changeAge(Student student) &#123; student.setAge(20); &#125; public static void main(String[] args) &#123; Student student = new Student(); student.setAge(18); changeAge(student); System.out.println(&quot;student.age=&quot; + student.getAge()); &#125; static class Student &#123; int age; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; &#125;&#125; 上面这段代码的输出结果是student.age=20。 main方法new Student时，jvm在堆中分配了一块内存用于存放Student对象。并且在栈中存放一个student对象，值是指向堆中Student对象的地址。 把student变量传入changeAge()方法时，jvm复制了一份student变量。为了便于理解我们叫它temp，temp的值和student是一样的，都是指向堆中Student对象的地址。 也就是说两个指针都指向了堆中Student对象的地址，此时修改的也就是堆中的该对象，因此main中的student对象也发生了变化 特殊的String 按照上面的说法，String不是基本类型，按理来说应该是引用传递 12345678910111213public class Test01 &#123; public static void change(String str) &#123; str = &quot;456&quot;; &#125; public static void main(String[] args) &#123; String str = &quot;123&quot;; change(str); System.out.println(&quot;str = &quot; + str); &#125;&#125; 实际上上面输出的结果是str = 123。 jvm在实例化字符串时会使用字符串常量池，把str作为参数传入change()方法。jvm复制了一份str变量，为了便于理解我们叫它temp。这个时候str和temp都指向字符串常量池中的&quot;123&quot;。 后续给temp重新赋值为&quot;456&quot;，可以理解为在常量池中新建了&quot;456&quot;这个字符串，并将temp指向常量池中的&quot;456&quot;，此时str的指向是没有发生变化的，因此还是&quot;123&quot;。","categories":[{"name":"Java","slug":"Java","permalink":"https://suavess.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://suavess.github.io/tags/Java/"}]}],"categories":[{"name":"分布式","slug":"分布式","permalink":"https://suavess.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"Java","slug":"Java","permalink":"https://suavess.github.io/categories/Java/"},{"name":"MySQL","slug":"MySQL","permalink":"https://suavess.github.io/categories/MySQL/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://suavess.github.io/categories/ElasticSearch/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://suavess.github.io/tags/Java/"},{"name":"分布式","slug":"分布式","permalink":"https://suavess.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"事务","slug":"事务","permalink":"https://suavess.github.io/tags/%E4%BA%8B%E5%8A%A1/"},{"name":"基础","slug":"基础","permalink":"https://suavess.github.io/tags/%E5%9F%BA%E7%A1%80/"},{"name":"关键字","slug":"关键字","permalink":"https://suavess.github.io/tags/%E5%85%B3%E9%94%AE%E5%AD%97/"},{"name":"反编译","slug":"反编译","permalink":"https://suavess.github.io/tags/%E5%8F%8D%E7%BC%96%E8%AF%91/"},{"name":"MySQL","slug":"MySQL","permalink":"https://suavess.github.io/tags/MySQL/"},{"name":"行锁","slug":"行锁","permalink":"https://suavess.github.io/tags/%E8%A1%8C%E9%94%81/"},{"name":"表锁","slug":"表锁","permalink":"https://suavess.github.io/tags/%E8%A1%A8%E9%94%81/"},{"name":"索引失效","slug":"索引失效","permalink":"https://suavess.github.io/tags/%E7%B4%A2%E5%BC%95%E5%A4%B1%E6%95%88/"},{"name":"触发器","slug":"触发器","permalink":"https://suavess.github.io/tags/%E8%A7%A6%E5%8F%91%E5%99%A8/"},{"name":"索引","slug":"索引","permalink":"https://suavess.github.io/tags/%E7%B4%A2%E5%BC%95/"},{"name":"数据结构","slug":"数据结构","permalink":"https://suavess.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://suavess.github.io/tags/ElasticSearch/"},{"name":"Docker","slug":"Docker","permalink":"https://suavess.github.io/tags/Docker/"},{"name":"锁机制","slug":"锁机制","permalink":"https://suavess.github.io/tags/%E9%94%81%E6%9C%BA%E5%88%B6/"},{"name":"集合","slug":"集合","permalink":"https://suavess.github.io/tags/%E9%9B%86%E5%90%88/"}]}